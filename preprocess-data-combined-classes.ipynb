{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "preprocess-data-combined-classes.py\n",
    "\n",
    "create train_dset, val_dset, and test_dset from Ogata et al. data\n",
    "\n",
    "Date        Time   Who      Updates\n",
    "----------  -----  -------  ----------------\n",
    "2020-11-16  20:00  Rachel   made this separate file to keep pickled files for combined class problem separate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import copy\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import random\n",
    "from numpy.random import seed\n",
    "from tensorflow.random import set_seed\n",
    "seed(0)\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing: Class Combination Case\n",
    "\n",
    "Read in data to generate Pandas dataframe. If generating for the first time, will take about five minutes. Otherwise, it should take about 10 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing ../data/Pose_Dataset/\n",
      "Done processing ../data/Pose_Dataset/good\n",
      "Done processing ../data/Pose_Dataset/good/1115_3djoints_index\n",
      "Done processing ../data/Pose_Dataset/bad_toe\n",
      "Done processing ../data/Pose_Dataset/bad_toe/1115_3djoints_index\n",
      "Done processing ../data/Pose_Dataset/bad_shallow\n",
      "Done processing ../data/Pose_Dataset/bad_shallow/1115_3djoints_index\n",
      "Done processing ../data/Pose_Dataset/bad_innner_thigh\n",
      "Done processing ../data/Pose_Dataset/bad_innner_thigh/1115_3djoints_index\n",
      "Done processing ../data/Pose_Dataset/bad_back_round\n",
      "Done processing ../data/Pose_Dataset/bad_back_round/1115_3djoints_index\n",
      "Done processing ../data/Pose_Dataset/bad_back_warp\n",
      "Done processing ../data/Pose_Dataset/bad_back_warp/1115_3djoints_index\n",
      "Done processing ../data/Pose_Dataset/bad_head\n",
      "Done processing ../data/Pose_Dataset/bad_head/1115_3djoints_index\n"
     ]
    }
   ],
   "source": [
    "datapath = '../data/Pose_Dataset/'\n",
    "nFrames = 300 # number of frames per squat\n",
    "df_filename = '../data/all-data-combined-classes.pkl' # where to save concatenated data\n",
    "np_filename = '../data/np-data-combined-classes.npy'\n",
    "name_to_label = {\n",
    "    'bad_innner_thigh': 0,\n",
    "    'bad_back_round': 1,\n",
    "    'bad_back_warp': 2,\n",
    "    'bad_head': 3,\n",
    "    'bad_shallow': 4,\n",
    "    'bad_toe': 5,\n",
    "    'good': 6\n",
    "}\n",
    "\n",
    "#error analysis utility: for combining certain classes to see if this improves accuracy of model...if not combining any classes, pass empty matrix []\n",
    "#classes_to_combine = [['bad_innner_thigh', 'bad_head']] \n",
    "#classes_to_combine = [['bad_head', 'bad_toe']] \n",
    "#classes_to_combine = [['bad_back_warp', 'bad_back_round'], ['bad_head', 'bad_toe']] \n",
    "#classes_to_combine = [['bad_back_warp', 'bad_back_round']]\n",
    "classes_to_combine = [['bad_back_warp', 'bad_back_round']]\n",
    "\n",
    "##have to comment out if through else opening and move the code below one tab to left if need to regenerate the pickled files.\n",
    "#do the opposite to use the saved data OR just delete the files and then regenerate them\n",
    "\n",
    "if os.path.exists(df_filename) and os.path.exists(np_filename):\n",
    "    # read in pkl file\n",
    "    df = pd.read_pickle(df_filename)\n",
    "    with open(np_filename, 'rb') as f:\n",
    "        X_train = np.load(f)\n",
    "        y_train = np.load(f)\n",
    "        X_val = np.load(f)\n",
    "        y_val = np.load(f)\n",
    "        X_test = np.load(f)\n",
    "        y_test = np.load(f)\n",
    "else:\n",
    "    # generate pkl file and npy file\n",
    "\n",
    "    # initialize arrays to fill in each loop iteration\n",
    "    filenames = []\n",
    "    datas = []\n",
    "    np_datas = []\n",
    "    labels = []\n",
    "    \n",
    "    import copy\n",
    "    # read in each squat file\n",
    "    for cur_dir, _, files in os.walk(datapath):\n",
    "        for file in files:\n",
    "            if not file.endswith('.json'):\n",
    "                continue\n",
    "            filename = os.path.join(cur_dir, file)\n",
    "            filenames.append(filename)\n",
    "\n",
    "            data = pd.read_json(filename).to_numpy()\n",
    "            data = data[1,0:nFrames] # get data for frames. Note some files have 301 frames, truncate all to first 300\n",
    "            datas.append(data)\n",
    "            np_data = np.array([np.array(d) for d in data])\n",
    "            if np_data.shape == (300, 171):\n",
    "                np_datas.append(np_data)\n",
    "                labels.append(name_to_label[cur_dir.split('/')[-2]]) #append label if appending data \n",
    "                #labels.append(name_to_label_comb[cur_dir.split('/')[-2]]) #replaced line above to check out the error analysis\n",
    "        print(f\"Done processing {cur_dir}\")\n",
    "        # move arrays into dataframe\n",
    "    datas = np.array(datas) # convert to array so we can loop through\n",
    "    d = {'filename': filenames}\n",
    "    for i in range(nFrames):\n",
    "        d[str(i)] = datas[:,i]\n",
    "    df = pd.DataFrame(data=d)\n",
    "\n",
    "    #add label column based on filename (also makes new naming for combined classes)\n",
    "    new_classes = ['_or_'.join(classes) for classes in classes_to_combine]\n",
    "    new_labels = []\n",
    "    for i in range(len(df)):\n",
    "        current_label = df['filename'][i].split('/')[3]\n",
    "        for j in range(len(classes_to_combine)):\n",
    "            if current_label in classes_to_combine[j]:\n",
    "                current_label = new_classes[j]\n",
    "        new_labels.append(current_label)\n",
    "    df['label'] = new_labels  \n",
    "    \n",
    "    # save to pkl\n",
    "    df.to_pickle(df_filename)\n",
    "\n",
    "    # write data into numpy arrays and save\n",
    "    np_datas = np.stack(np_datas)\n",
    "    labels = np.array(labels)\n",
    "    \n",
    "    #separate data into classes \n",
    "    num_classes = len(name_to_label)\n",
    "    pre_data = []\n",
    "    pre_labels = []   \n",
    "    for i in range(num_classes):\n",
    "        indices = np.where(labels == i) #finding indices where particular class is found in labels\n",
    "        pre_labels.append(labels[indices])\n",
    "        pre_data.append(np_datas[indices])\n",
    "        \n",
    "    #choose to keep only half of the data from the 2 classes we're combining\n",
    "    modified_name_to_label = copy.deepcopy(name_to_label)\n",
    "\n",
    "    for i in range(len(classes_to_combine)):\n",
    "        for j in classes_to_combine[i]:\n",
    "            label = name_to_label[j]\n",
    "            label_to_use = name_to_label[ classes_to_combine[i][0] ] #choosing the first label in the group of classes to combine\n",
    "            modified_name_to_label[j] = label_to_use\n",
    "\n",
    "            fraction = 1/len(classes_to_combine[i]) #if don't want to reduce data such that data in the combined class is roughly equal to data in the other non-combined classes, make fraction = 1\n",
    "            pre_data[label] = pre_data[label][0:int(fraction*len( pre_data[label]))]\n",
    "            pre_labels[label] = [label_to_use]*int(fraction*len( pre_labels[label])) #replacing original label with the new one based on the combined classes\n",
    "\n",
    "    #recombine the pre-data and pre-labels\n",
    "    np_datas_combined = [] \n",
    "    labels_combined = []\n",
    "\n",
    "    for i in range(len(pre_labels)):\n",
    "        np_datas_combined.extend(pre_data[i])\n",
    "        labels_combined.extend(pre_labels[i])\n",
    "    np_datas_combined = np.array(np_datas_combined)\n",
    "    labels_combined =   np.array(labels_combined)  \n",
    "\n",
    "    #shuffle the data\n",
    "    np_datas, labels = shuffle(np_datas_combined, labels_combined) #note this is using the combined frames....not the original non-combined ones  \n",
    "\n",
    "    #modify name_to_label and labels to account for the renumbered classes\n",
    "    def reorder(name_to_label, labels ):\n",
    "        #create a set from labels to get unique set of labels\n",
    "        sorted_unique_labels = list( set(labels) )\n",
    "        sorted_unique_labels.sort() \n",
    "\n",
    "\n",
    "        for i in range(len(sorted_unique_labels)):\n",
    "            for key,value in name_to_label.items():\n",
    "                if value == sorted_unique_labels[i]:\n",
    "                    name_to_label[key] = i #adjust the value in the name to label dict\n",
    "                    labels = np.where(labels==value, i, labels) #adjust the value in labels\n",
    "        return name_to_label, labels\n",
    "    \n",
    "    name_to_label, labels =  reorder(modified_name_to_label, labels)\n",
    "    \n",
    "    \n",
    "    #original splitting of train, val, test\n",
    "    n = np_datas.shape[0]\n",
    "    split_indices = [int(n * 0.8), int(n * 0.9)]\n",
    "    train_indices, val_indices, test_indices = np.split(np.random.choice(n, n, replace=False), split_indices)\n",
    "\n",
    "    X_train = np_datas[train_indices]\n",
    "    y_train = labels[train_indices]\n",
    "\n",
    "    X_val = np_datas[val_indices]\n",
    "    y_val = labels[val_indices]\n",
    "\n",
    "    X_test = np_datas[test_indices]\n",
    "    y_test = labels[test_indices]\n",
    "\n",
    "    with open(np_filename, 'wb') as f:\n",
    "        np.save(f, X_train)\n",
    "        np.save(f, y_train)\n",
    "        np.save(f, X_val)\n",
    "        np.save(f, y_val)\n",
    "        np.save(f, X_test)\n",
    "        np.save(f, y_test)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print number of examples in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good: 293\n",
      "bad_toe: 295\n",
      "bad_shallow: 319\n",
      "bad_innner_thigh: 230\n",
      "bad_back_warp_or_bad_back_round: 592\n",
      "bad_head: 272\n",
      "nClasses: 6\n",
      "\n",
      " Train\n",
      "0: 177\n",
      "1: 238\n",
      "2: 217\n",
      "3: 265\n",
      "4: 229\n",
      "5: 237\n",
      "\n",
      " Val\n",
      "0: 29\n",
      "1: 34\n",
      "2: 23\n",
      "3: 26\n",
      "4: 31\n",
      "5: 27\n",
      "\n",
      " Test\n",
      "0: 23\n",
      "1: 24\n",
      "2: 32\n",
      "3: 28\n",
      "4: 35\n",
      "5: 29\n"
     ]
    }
   ],
   "source": [
    "df_labels = df.label.unique()\n",
    "for label in df_labels:\n",
    "    print('%s: %i' %(label,sum(df['label']==label)))\n",
    "\n",
    "nClasses = len(df_labels)\n",
    "print('nClasses:', nClasses)\n",
    "\n",
    "print('\\n Train')\n",
    "df_labels = df.label.unique()\n",
    "for i in range(nClasses):\n",
    "    print('%s: %i' %(i,sum(y_train==i)))\n",
    "\n",
    "print('\\n Val')\n",
    "df_labels = df.label.unique()\n",
    "for i in range(nClasses):\n",
    "    print('%s: %i' %(i,sum(y_val==i)))\n",
    "\n",
    "print('\\n Test')\n",
    "df_labels = df.label.unique()\n",
    "for i in range(nClasses):\n",
    "    print('%s: %i' %(i,sum(y_test==i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert y-vals to one-hot representation # REMEMBER TO ONLY RUN THIS ONCE\n",
    "y_train_onehot = tf.keras.utils.to_categorical(y_train,num_classes=nClasses)\n",
    "y_val_onehot = tf.keras.utils.to_categorical(y_val,num_classes=nClasses)\n",
    "y_test_onehot = tf.keras.utils.to_categorical(y_test,num_classes=nClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for BATCH_SIZE in [16, 64, 128]:\n",
    "    train_dset = tf.data.Dataset.from_tensor_slices((X_train,y_train_onehot)).batch(BATCH_SIZE)\n",
    "    val_dset = tf.data.Dataset.from_tensor_slices((X_val,y_val_onehot)).batch(BATCH_SIZE)\n",
    "    test_dset = tf.data.Dataset.from_tensor_slices((X_test,y_test_onehot)).batch(BATCH_SIZE)\n",
    "    tf.data.experimental.save(train_dset,f'../data/dsets_combined_classes/train_dset-{BATCH_SIZE}')\n",
    "    tf.data.experimental.save(val_dset,f'../data/dsets_combined_classes/val_dset-{BATCH_SIZE}')\n",
    "    tf.data.experimental.save(test_dset,f'../data/dsets_combined_classes/test_dset-{BATCH_SIZE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save datasets\n",
    "# tf.data.experimental.save(train_dset,'../data/dsets_combined_classes/train_dset')\n",
    "# tf.data.experimental.save(val_dset,'../data/dsets_combined_classes/val_dset')\n",
    "# tf.data.experimental.save(test_dset,'../data/dsets_combined_classes/test_dset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=(None, 300, 171), dtype=tf.float64, name=None),\n",
       " TensorSpec(shape=(None, 6), dtype=tf.float32, name=None))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print element_spec for input to loading model in other notebooks\n",
    "train_dset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good' 'bad_toe' 'bad_shallow' 'bad_innner_thigh'\n",
      " 'bad_back_warp_or_bad_back_round' 'bad_head'] \n",
      " {'bad_innner_thigh': 0, 'bad_back_round': 1, 'bad_back_warp': 1, 'bad_head': 2, 'bad_shallow': 3, 'bad_toe': 4, 'good': 5}\n"
     ]
    }
   ],
   "source": [
    "##variables to use for the confusion matrices\n",
    "print(df_labels,'\\n', name_to_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "display_labels = [] list is df_labels but ordering must be adjusted to go from smallest to largest  based on the values in name_to_label\n",
    "\n",
    "ex. for df_labels = ['good' 'bad_toe' 'bad_shallow' 'bad_innner_thigh_or_bad_head'\n",
    " 'bad_back_round' 'bad_back_warp']\n",
    " \n",
    " and name_to_label = {'bad_innner_thigh': 0, 'bad_back_round': 1, 'bad_back_warp': 2, 'bad_head': 0, 'bad_shallow': 3, 'bad_toe': 4, 'good': 5}\n",
    " \n",
    " display_labels = ['bad_innner_thigh_or_bad_head', 'bad_back_round', 'bad_back_warp', 'bad_shallow', 'bad_toe', 'good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_labels = ['bad_inner_thigh', 'bad_back_round', 'bad_back_warp', 'bad_head', 'bad_shallow', 'bad_toe', 'good']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

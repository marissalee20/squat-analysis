{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "preprocess-data.py\n",
    "\n",
    "create train_dset, val_dset, and test_dset from Ogata et al. data\n",
    "\n",
    "Date        Time   Who      Updates\n",
    "----------  -----  -------  ----------------\n",
    "2020-11-11  11:36  Marissa  move preprocessing from ogata-model.ipynb and lstm-model.ipynb; save datasets\n",
    "2020-11-15  22:00  Rachel   make adjustments to allow for combining multiple classes (used for error analysis)\n",
    "2020-11-16  20:00  Rachel   made copy of this file for combined class problem in the preprocess-data-combined-classes                             file so that pickled files are kept separate\n",
    "2020-11-16  21:00  Blake    make train/val/test sets with different batch sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import copy\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import random\n",
    "from numpy.random import seed\n",
    "from tensorflow.random import set_seed\n",
    "seed(0)\n",
    "set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing: No Class Combination Case\n",
    "\n",
    "Read in data to generate Pandas dataframe. If generating for the first time, will take about five minutes. Otherwise, it should take about 10 seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done processing ../data/Pose_Dataset/\n",
      "Done processing ../data/Pose_Dataset/good\n",
      "Done processing ../data/Pose_Dataset/good/1115_3djoints_index\n",
      "Done processing ../data/Pose_Dataset/bad_toe\n",
      "Done processing ../data/Pose_Dataset/bad_toe/1115_3djoints_index\n",
      "Done processing ../data/Pose_Dataset/bad_shallow\n",
      "Done processing ../data/Pose_Dataset/bad_shallow/1115_3djoints_index\n",
      "Done processing ../data/Pose_Dataset/bad_innner_thigh\n",
      "Done processing ../data/Pose_Dataset/bad_innner_thigh/1115_3djoints_index\n",
      "Done processing ../data/Pose_Dataset/bad_back_round\n",
      "Done processing ../data/Pose_Dataset/bad_back_round/1115_3djoints_index\n",
      "Done processing ../data/Pose_Dataset/bad_back_warp\n",
      "Done processing ../data/Pose_Dataset/bad_back_warp/1115_3djoints_index\n",
      "Done processing ../data/Pose_Dataset/bad_head\n",
      "Done processing ../data/Pose_Dataset/bad_head/1115_3djoints_index\n"
     ]
    }
   ],
   "source": [
    "datapath = '../data/Pose_Dataset/'\n",
    "nFrames = 300 # number of frames per squat\n",
    "df_filename = '../data/all-data.pkl' # where to save concatenated data\n",
    "np_filename = '../data/np-data.npy'\n",
    "name_to_label = {\n",
    "    'bad_innner_thigh': 0,\n",
    "    'bad_back_round': 1,\n",
    "    'bad_back_warp': 2,\n",
    "    'bad_head': 3,\n",
    "    'bad_shallow': 4,\n",
    "    'bad_toe': 5,\n",
    "    'good': 6\n",
    "}\n",
    "\n",
    "if os.path.exists(df_filename) and os.path.exists(np_filename):\n",
    "    # read in pkl file\n",
    "    df = pd.read_pickle(df_filename)\n",
    "    with open(np_filename, 'rb') as f:\n",
    "        X_train = np.load(f)\n",
    "        y_train = np.load(f)\n",
    "        X_val = np.load(f)\n",
    "        y_val = np.load(f)\n",
    "        X_test = np.load(f)\n",
    "        y_test = np.load(f)\n",
    "else:\n",
    "    # generate pkl file and npy file\n",
    "\n",
    "    # initialize arrays to fill in each loop iteration\n",
    "    filenames = []\n",
    "    datas = []\n",
    "    np_datas = []\n",
    "    labels = []\n",
    "\n",
    "    # read in each squat file\n",
    "    for cur_dir, _, files in os.walk(datapath):\n",
    "        for file in files:\n",
    "            if not file.endswith('.json'):\n",
    "                continue\n",
    "            filename = os.path.join(cur_dir, file)\n",
    "            filenames.append(filename)\n",
    "\n",
    "            data = pd.read_json(filename).to_numpy()\n",
    "            data = data[1,0:nFrames] # get data for frames. Note some files have 301 frames, truncate all to first 300\n",
    "            datas.append(data)\n",
    "            np_data = np.array([np.array(d) for d in data])\n",
    "            if np_data.shape == (300, 171):\n",
    "                np_datas.append(np_data)\n",
    "            labels.append(name_to_label[cur_dir.split('/')[-2]])\n",
    "        print(f\"Done processing {cur_dir}\")\n",
    "\n",
    "    # move arrays into dataframe\n",
    "    datas = np.array(datas) # convert to array so we can loop through\n",
    "    d = {'filename': filenames}\n",
    "    for i in range(nFrames):\n",
    "        d[str(i)] = datas[:,i]\n",
    "    df = pd.DataFrame(data=d)\n",
    "\n",
    "    # save to pkl\n",
    "    df.to_pickle(df_filename)\n",
    "\n",
    "    # write data into numpy arrays and save\n",
    "    np_datas = np.stack(np_datas)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    n = np_datas.shape[0]\n",
    "    split_indices = [int(n * 0.8), int(n * 0.9)]\n",
    "    train_indices, val_indices, test_indices = np.split(np.random.choice(n, n, replace=False), split_indices)\n",
    "\n",
    "    X_train = np_datas[train_indices]\n",
    "    y_train = labels[train_indices]\n",
    "\n",
    "    X_val = np_datas[val_indices]\n",
    "    y_val = labels[val_indices]\n",
    "\n",
    "    X_test = np_datas[test_indices]\n",
    "    y_test = labels[test_indices]\n",
    "    \n",
    "    with open(np_filename, 'wb') as f:\n",
    "        np.save(f, X_train)\n",
    "        np.save(f, y_train)\n",
    "        np.save(f, X_val)\n",
    "        np.save(f, y_val)\n",
    "        np.save(f, X_test)\n",
    "        np.save(f, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the first five rows of the dataframe. The 'filename' column contains full filepath + name of each squat, and the remaining columns contain pose data for each of 300 frames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print number of examples in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = df.label.unique()\n",
    "for label in df_labels:\n",
    "    print('%s: %i' %(label,sum(df['label']==label)))\n",
    "\n",
    "nClasses = len(df_labels)\n",
    "print('nClasses:', nClasses)\n",
    "\n",
    "print('\\n Train')\n",
    "df_labels = df.label.unique()\n",
    "for i in range(nClasses):\n",
    "    print('%s: %i' %(i,sum(y_train==i)))\n",
    "\n",
    "print('\\n Val')\n",
    "df_labels = df.label.unique()\n",
    "for i in range(nClasses):\n",
    "    print('%s: %i' %(i,sum(y_val==i)))\n",
    "\n",
    "print('\\n Test')\n",
    "df_labels = df.label.unique()\n",
    "for i in range(nClasses):\n",
    "    print('%s: %i' %(i,sum(y_test==i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert y-vals to one-hot representation # REMEMBER TO ONLY RUN THIS ONCE\n",
    "y_train_onehot = tf.keras.utils.to_categorical(y_train,num_classes=nClasses)\n",
    "y_val_onehot = tf.keras.utils.to_categorical(y_val,num_classes=nClasses)\n",
    "y_test_onehot = tf.keras.utils.to_categorical(y_test,num_classes=nClasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "train_dset = tf.data.Dataset.from_tensor_slices((X_train,y_train_onehot)).batch(BATCH_SIZE)\n",
    "val_dset = tf.data.Dataset.from_tensor_slices((X_val,y_val_onehot)).batch(BATCH_SIZE)\n",
    "test_dset = tf.data.Dataset.from_tensor_slices((X_test,y_test_onehot)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save datasets\n",
    "tf.data.experimental.save(train_dset,'../data/dsets/train_dset')\n",
    "tf.data.experimental.save(val_dset,'../data/dsets/val_dset')\n",
    "tf.data.experimental.save(test_dset,'../data/dsets/test_dset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print element_spec for input to loading model in other notebooks\n",
    "train_dset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##variables to use for the confusion matrices\n",
    "print(df_labels,'\\n', name_to_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "display_labels = [] list is df_labels but ordering must be adjusted to go from smallest to largest  based on the values in name_to_label\n",
    "\n",
    "ex. for df_labels = ['good' 'bad_toe' 'bad_shallow' 'bad_innner_thigh_or_bad_head'\n",
    " 'bad_back_round' 'bad_back_warp']\n",
    " \n",
    " and name_to_label = {'bad_innner_thigh': 0, 'bad_back_round': 1, 'bad_back_warp': 2, 'bad_head': 0, 'bad_shallow': 3, 'bad_toe': 4, 'good': 5}\n",
    " \n",
    " display_labels = ['bad_innner_thigh_or_bad_head', 'bad_back_round', 'bad_back_warp', 'bad_shallow', 'bad_toe', 'good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_labels = ['bad_inner_thigh', 'bad_back_warp_or_bad_back_round', 'bad_head', 'bad_shallow', 'bad_toe', 'good']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
